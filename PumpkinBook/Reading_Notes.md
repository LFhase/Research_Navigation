# PumpkinBook

> My reading notes and selected answers of the [PumpkinBook](<http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm>)



## Chapter 0 Mathmatical Basics

- 函数求导、偏导、链导
- 矩阵正定
- 矩阵微分



## Chapter 1 Introduction

- Basic Concepts: balabala
- 奥卡姆剃刀原理
- No Free Lunch Theorem



## Chapter 2 模型评估与选择



#### 直观的

- Accuracy
- Error: 训练/经验-训练集， 泛化误差-新样本， 测试误差-测试集



####　评估方法

- Hold-out
  - 数据集= 训练集+测试集
  - **训练/测试集的划分应该尽可能保证数据分布的一致性**
- Cross validation
  - 划分成k个大小相似的互斥子集，分层采样
  - 留一法
- Bootstrapping，自助法
  - 自助采样：有放回、等概率采样
  - $\lim_{m\to\infty} (1-\frac{1}{m})^m \to \frac{1}{e} $，约有$\frac{1}{3}$的数据被留作测试集
  - 在数据集较小、难以有效划分训练/测试集时很有用
  - 改变了初始数据集分布，引入估计偏差



#### 性能度量

度量泛化能力

- 错误率和精度
- 查准率、查全率与F1
  - 查准率/准确率：P/(TP+FP)
  - 查全率/召回率：TP/(TP+FN)
  - P-R 曲线：按h(x)从大到小排序，依次计算P，R
  - 平衡点“Break-Even Point”：P=R
  - F1:2PR/（P+R）
- ROC与AUC：
  - ROC曲线：TPR（真正例率）-FPR（假正例率）
  - 分类阈值从小到大绘制
  - AUC：ROC下面积



#### 比较检验

- 二项检验
- T检验



#### 方差与偏差

- 



## Chapter 3 线性模型

#### 线性回归

- 什么情况下，可以不必考虑偏置项？
  - 偏置项的变化体现出来的就是拟合曲线的上下整体浮动，可以看做是其他各个解释变量留下的bias的线性修正
  - 如果目标变量均值为0，或者只考虑不同自变量对因变量的影响，则不需要考虑偏置项
- 证明对率回归的目标函数$y = \frac{1}{1+e^{-w^tx+b}} $是非凸的，但其对数似然函数是凸的
  - 凸函数的定义：
    - 二阶导数>=0
    - $$f[\frac{(x_1+x_2)}{2}] < = \frac{[f(x_1)+f(x_2)]}{2}$$   
  - 目标函数二阶导数为$(1-2y)y(1-y)w^tw$，因为$w^tw$是半正定的，因此在y>0.5时，导数<0，因此非凸
  - 后者简单

***remark***: 因为凸函数，所以可以通过令导数=0,求得最优解

#### 对数几率回归

***remark***: 逻辑回归

#### LDA

***remark***: 类似于一维的SVM，如果一维线性不可分，就通过函数映射到高维空间，使其可分

#### 类别不平衡问题

- 欠采样
- 过采样
- 阈值移动



## Chapter 4 决策树

- 训练算法

#### 划分选择

- 信息增益：
  - 信息熵-ID3
  - 增益率-C4.5，对属性可能取值加以惩罚
  - 基尼系数-CART

#### 剪枝

- 预剪枝
- 后剪枝

#### 连续与缺失值

- 连续值：二分
- 缺失值：
  - 确定划分属性时，只计算完整样本的增益*样本完整率
  - 给定属性时，用非缺失值在对应属性占比分割该样本

#### 多变量决策树

- 决策树形成的边界与样本空间坐标轴平行
- 分类边界复杂时会使得决策树也很复杂
- 多变量决策树每个节点都是一个$\sum^d_{i=1}w_ia_i = t$的线性分类器，使其分类边界为一条斜线

#### 课后题

**4.1**

训练误差产生的原因是存在冲突数据，即在某个节点下存在属性取值相同但label不同的数据

**4.2**

可能会让模型过拟合至训练数据的一些未知“模式”，导致泛化能力较弱

**4.9**

很直观

### 

## Chapter 5 神经网络

#### 注意点

- 感知机只能解决线性可分的问题
- 只需要一个包含足够多神经元的隐藏层，就可以拟合任意复杂度的连续函数
- **离散属性在输入时，如果不存在序关系，需要做成一个向量**
- 跳出局部最优解
  - 多组不同参数随机初始化多个模型，选择最好的
  - 模拟退火
  - 随机梯度下降

#### 课后题

**5.1**

使得神经网络退化成为线性回归

**5.2**

对应上一层，下一层的每个神经元都可以看作是一个对率回归





## Chapter 6 SVM



